{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classificador de notícias","metadata":{}},{"cell_type":"markdown","source":"## Objetivo","metadata":{}},{"cell_type":"markdown","source":"Utilizando o modelo \"microsoft/deberta-v3-small\" como base para treinar um modelo classificador de notícias, entre os seguintes tipos World, Sports, Business e Sci/Tech. Inspirado no modelo \"wesleyacheng/news-topic-classification-with-bert\".","metadata":{}},{"cell_type":"markdown","source":"## Instalação\n","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers","metadata":{"execution":{"iopub.status.busy":"2023-10-26T17:53:53.756984Z","iopub.execute_input":"2023-10-26T17:53:53.757290Z","iopub.status.idle":"2023-10-26T17:54:04.021034Z","shell.execute_reply.started":"2023-10-26T17:53:53.757252Z","shell.execute_reply":"2023-10-26T17:54:04.020019Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.18.4)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.16.2)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.26.0)\nRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.62.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.20.3)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.12.2)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.4.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.2.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.11.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.49)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.0.9)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Frames","metadata":{}},{"cell_type":"markdown","source":"Realizar a importação do dataset, o utilizado foi o [\"ag_news\"](https://huggingface.co/datasets/ag_news) vindo do hugging face.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"ag_news\")\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-10-26T17:54:04.022981Z","iopub.execute_input":"2023-10-26T17:54:04.023253Z","iopub.status.idle":"2023-10-26T17:54:04.592466Z","shell.execute_reply.started":"2023-10-26T17:54:04.023222Z","shell.execute_reply":"2023-10-26T17:54:04.591659Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a95cee0196bd4349afb5ed84e871624e"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 120000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 7600\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenização","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification,AutoTokenizer\nmodel_nm = 'microsoft/deberta-v3-small'\ntokenizer = AutoTokenizer.from_pretrained(model_nm)","metadata":{"execution":{"iopub.status.busy":"2023-10-26T17:54:04.593565Z","iopub.execute_input":"2023-10-26T17:54:04.593781Z","iopub.status.idle":"2023-10-26T17:54:07.232984Z","shell.execute_reply.started":"2023-10-26T17:54:04.593756Z","shell.execute_reply":"2023-10-26T17:54:07.232177Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\nloading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\nloading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nAdding [MASK] to the vocabulary\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Foi necessário renomear a coluna 'label'.","metadata":{}},{"cell_type":"code","source":"def tokenizerFunc(x): return tokenizer(x[\"text\"])\ntok_ds = dataset.map(tokenizerFunc, batched=True);\ntok_ds = tok_ds.rename_column('label', 'labels')\ntok_ds","metadata":{"execution":{"iopub.status.busy":"2023-10-26T17:54:07.234799Z","iopub.execute_input":"2023-10-26T17:54:07.235051Z","iopub.status.idle":"2023-10-26T17:54:10.357123Z","shell.execute_reply.started":"2023-10-26T17:54:07.235020Z","shell.execute_reply":"2023-10-26T17:54:10.356287Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 120000\n    })\n    test: Dataset({\n        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 7600\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Também foi necessário utilizar um tamanho menor do dataset para evitar problemas de memória, no caso foi utilizado 60% do dataset original.","metadata":{}},{"cell_type":"code","source":"fraction = 0.6 \nnum_train_examples = int(len(tok_ds['train']) * fraction)\nnum_test_examples = int(len(tok_ds['test']) * fraction)\nnum_validation_examples = int(len(tok_ds['train']) * fraction) \n\ntok_ds['train'] = tok_ds['train'].select(range(num_train_examples))\ntok_ds['test'] = tok_ds['test'].select(range(num_test_examples))\ntok_ds['validation'] = tok_ds['train'].select(range(num_validation_examples))","metadata":{"execution":{"iopub.status.busy":"2023-10-26T17:54:10.358353Z","iopub.execute_input":"2023-10-26T17:54:10.358624Z","iopub.status.idle":"2023-10-26T17:54:10.398166Z","shell.execute_reply.started":"2023-10-26T17:54:10.358578Z","shell.execute_reply":"2023-10-26T17:54:10.397540Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Treinamento do modelo","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments,Trainer","metadata":{"execution":{"iopub.status.busy":"2023-10-26T17:54:10.399329Z","iopub.execute_input":"2023-10-26T17:54:10.399887Z","iopub.status.idle":"2023-10-26T17:54:10.404025Z","shell.execute_reply.started":"2023-10-26T17:54:10.399827Z","shell.execute_reply":"2023-10-26T17:54:10.403284Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"bs = 50\nepochs = 4\nlr = 8e-5","metadata":{"execution":{"iopub.status.busy":"2023-10-26T17:54:10.405129Z","iopub.execute_input":"2023-10-26T17:54:10.405347Z","iopub.status.idle":"2023-10-26T17:54:10.413558Z","shell.execute_reply.started":"2023-10-26T17:54:10.405320Z","shell.execute_reply":"2023-10-26T17:54:10.412954Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=0.01, report_to='none')","metadata":{"execution":{"iopub.status.busy":"2023-10-26T17:54:10.414435Z","iopub.execute_input":"2023-10-26T17:54:10.414639Z","iopub.status.idle":"2023-10-26T17:54:10.431724Z","shell.execute_reply.started":"2023-10-26T17:54:10.414614Z","shell.execute_reply":"2023-10-26T17:54:10.431016Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"}]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=4)\ntrainer = Trainer(model, args, train_dataset=tok_ds['train'], eval_dataset=tok_ds['test'], tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-10-26T17:54:10.432645Z","iopub.execute_input":"2023-10-26T17:54:10.432905Z","iopub.status.idle":"2023-10-26T17:54:13.660234Z","shell.execute_reply.started":"2023-10-26T17:54:10.432876Z","shell.execute_reply":"2023-10-26T17:54:13.659493Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\nSome weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing amp half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-26T17:54:13.662967Z","iopub.execute_input":"2023-10-26T17:54:13.663239Z","iopub.status.idle":"2023-10-26T18:38:33.511904Z","shell.execute_reply.started":"2023-10-26T17:54:13.663208Z","shell.execute_reply":"2023-10-26T18:38:33.511088Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"The following columns in the training set  don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 72000\n  Num Epochs = 4\n  Instantaneous batch size per device = 50\n  Total train batch size (w. parallel, distributed & accumulation) = 50\n  Gradient Accumulation steps = 1\n  Total optimization steps = 5760\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5760' max='5760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5760/5760 44:19, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.265000</td>\n      <td>0.222356</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.164300</td>\n      <td>0.177723</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.098100</td>\n      <td>0.199230</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.061600</td>\n      <td>0.212139</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to outputs/checkpoint-500\nConfiguration saved in outputs/checkpoint-500/config.json\nModel weights saved in outputs/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-500/special_tokens_map.json\nadded tokens file saved in outputs/checkpoint-500/added_tokens.json\n/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n  args.max_grad_norm,\nSaving model checkpoint to outputs/checkpoint-1000\nConfiguration saved in outputs/checkpoint-1000/config.json\nModel weights saved in outputs/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-1000/special_tokens_map.json\nadded tokens file saved in outputs/checkpoint-1000/added_tokens.json\nThe following columns in the evaluation set  don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text.\n***** Running Evaluation *****\n  Num examples = 4560\n  Batch size = 100\nSaving model checkpoint to outputs/checkpoint-1500\nConfiguration saved in outputs/checkpoint-1500/config.json\nModel weights saved in outputs/checkpoint-1500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-1500/special_tokens_map.json\nadded tokens file saved in outputs/checkpoint-1500/added_tokens.json\nSaving model checkpoint to outputs/checkpoint-2000\nConfiguration saved in outputs/checkpoint-2000/config.json\nModel weights saved in outputs/checkpoint-2000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-2000/special_tokens_map.json\nadded tokens file saved in outputs/checkpoint-2000/added_tokens.json\nSaving model checkpoint to outputs/checkpoint-2500\nConfiguration saved in outputs/checkpoint-2500/config.json\nModel weights saved in outputs/checkpoint-2500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-2500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-2500/special_tokens_map.json\nadded tokens file saved in outputs/checkpoint-2500/added_tokens.json\nThe following columns in the evaluation set  don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text.\n***** Running Evaluation *****\n  Num examples = 4560\n  Batch size = 100\nSaving model checkpoint to outputs/checkpoint-3000\nConfiguration saved in outputs/checkpoint-3000/config.json\nModel weights saved in outputs/checkpoint-3000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-3000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-3000/special_tokens_map.json\nadded tokens file saved in outputs/checkpoint-3000/added_tokens.json\n/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n  args.max_grad_norm,\nSaving model checkpoint to outputs/checkpoint-3500\nConfiguration saved in outputs/checkpoint-3500/config.json\nModel weights saved in outputs/checkpoint-3500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-3500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-3500/special_tokens_map.json\nadded tokens file saved in outputs/checkpoint-3500/added_tokens.json\nSaving model checkpoint to outputs/checkpoint-4000\nConfiguration saved in outputs/checkpoint-4000/config.json\nModel weights saved in outputs/checkpoint-4000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-4000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-4000/special_tokens_map.json\nadded tokens file saved in outputs/checkpoint-4000/added_tokens.json\nThe following columns in the evaluation set  don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text.\n***** Running Evaluation *****\n  Num examples = 4560\n  Batch size = 100\nSaving model checkpoint to outputs/checkpoint-4500\nConfiguration saved in outputs/checkpoint-4500/config.json\nModel weights saved in outputs/checkpoint-4500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-4500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-4500/special_tokens_map.json\nadded tokens file saved in outputs/checkpoint-4500/added_tokens.json\nSaving model checkpoint to outputs/checkpoint-5000\nConfiguration saved in outputs/checkpoint-5000/config.json\nModel weights saved in outputs/checkpoint-5000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-5000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-5000/special_tokens_map.json\nadded tokens file saved in outputs/checkpoint-5000/added_tokens.json\n/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n  args.max_grad_norm,\nSaving model checkpoint to outputs/checkpoint-5500\nConfiguration saved in outputs/checkpoint-5500/config.json\nModel weights saved in outputs/checkpoint-5500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-5500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-5500/special_tokens_map.json\nadded tokens file saved in outputs/checkpoint-5500/added_tokens.json\nThe following columns in the evaluation set  don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text.\n***** Running Evaluation *****\n  Num examples = 4560\n  Batch size = 100\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5760, training_loss=0.16805264088842603, metrics={'train_runtime': 2659.7917, 'train_samples_per_second': 108.279, 'train_steps_per_second': 2.166, 'total_flos': 9234269808673200.0, 'train_loss': 0.16805264088842603, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Testando os resultados do modelo","metadata":{}},{"cell_type":"code","source":"preds = trainer.predict(tok_ds['validation']).predictions.astype(float);\npreds","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:38:41.332990Z","iopub.execute_input":"2023-10-26T18:38:41.333316Z","iopub.status.idle":"2023-10-26T18:42:26.793016Z","shell.execute_reply.started":"2023-10-26T18:38:41.333284Z","shell.execute_reply":"2023-10-26T18:42:26.792247Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text.\n***** Running Prediction *****\n  Num examples = 72000\n  Batch size = 100\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [720/720 03:45]\n    </div>\n    "},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"array([[-0.82421875, -3.57226562,  5.87890625, -1.23242188],\n       [-1.29101562, -3.49804688,  6.02734375, -1.046875  ],\n       [-0.17114258, -3.85742188,  5.76953125, -1.45996094],\n       ...,\n       [ 6.2890625 , -2.91796875, -1.92285156, -2.046875  ],\n       [ 5.90234375, -3.265625  , -1.39160156, -1.70214844],\n       [ 6.05078125, -3.23046875, -1.60058594, -1.72265625]])"},"metadata":{}}]},{"cell_type":"code","source":"preds[:10]","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:42:42.106310Z","iopub.execute_input":"2023-10-26T18:42:42.106565Z","iopub.status.idle":"2023-10-26T18:42:42.112598Z","shell.execute_reply.started":"2023-10-26T18:42:42.106538Z","shell.execute_reply":"2023-10-26T18:42:42.111879Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"array([[-0.82421875, -3.57226562,  5.87890625, -1.23242188],\n       [-1.29101562, -3.49804688,  6.02734375, -1.046875  ],\n       [-0.17114258, -3.85742188,  5.76953125, -1.45996094],\n       [ 2.88671875, -4.19140625,  3.26953125, -1.63769531],\n       [ 3.38671875, -4.16015625,  2.87890625, -1.89550781],\n       [-1.25097656, -3.59765625,  6.10546875, -1.1015625 ],\n       [-1.4296875 , -3.33789062,  6.5546875 , -1.59082031],\n       [-1.12890625, -3.47460938,  6.30859375, -1.47753906],\n       [-1.109375  , -3.421875  ,  6.05078125, -1.25878906],\n       [-1.38769531, -3.31640625,  6.3515625 , -1.3984375 ]])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Transformando em zip para subir para o hugging face","metadata":{}},{"cell_type":"code","source":"save_directory = \"./news\"\ntokenizer.save_pretrained(save_directory);\ntrainer.save_model(save_directory);","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:57:29.581400Z","iopub.execute_input":"2023-10-26T18:57:29.581676Z","iopub.status.idle":"2023-10-26T18:57:31.019757Z","shell.execute_reply.started":"2023-10-26T18:57:29.581644Z","shell.execute_reply":"2023-10-26T18:57:31.018799Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"tokenizer config file saved in ./news/tokenizer_config.json\nSpecial tokens file saved in ./news/special_tokens_map.json\nadded tokens file saved in ./news/added_tokens.json\nSaving model checkpoint to ./news\nConfiguration saved in ./news/config.json\nModel weights saved in ./news/pytorch_model.bin\ntokenizer config file saved in ./news/tokenizer_config.json\nSpecial tokens file saved in ./news/special_tokens_map.json\nadded tokens file saved in ./news/added_tokens.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import zipfile\nwith zipfile.ZipFile(\"news.zip\", 'w',zipfile.ZIP_DEFLATED) as zipf:\n    for root, _, files in os.walk(save_directory):\n        for file in files:\n            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), save_directory))","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:57:48.101148Z","iopub.execute_input":"2023-10-26T18:57:48.101430Z","iopub.status.idle":"2023-10-26T18:58:19.405596Z","shell.execute_reply.started":"2023-10-26T18:57:48.101397Z","shell.execute_reply":"2023-10-26T18:58:19.404715Z"},"trusted":true},"execution_count":53,"outputs":[]}]}